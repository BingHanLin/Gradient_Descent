{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.4.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "source": [
    "## 循環神經網路（RNN）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 載入資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self):\n",
    "        path = tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "\n",
    "        with open(path, encoding='utf-8') as f:\n",
    "            self.raw_text = f.read().lower()\n",
    "\n",
    "        self.chars = sorted(list(set(self.raw_text)))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        self.text = [self.char_indices[c] for c in self.raw_text]\n",
    "\n",
    "        print (self.indices_char)\n",
    "        print (len(self.indices_char))\n",
    "\n",
    "        \n",
    "    def get_batch(self, seq_length, batch_size):\n",
    "        seq = []\n",
    "        next_char = []\n",
    "        for i in range(batch_size):\n",
    "            index = np.random.randint(0, len(self.text) - seq_length)\n",
    "            seq.append(self.text[index:index+seq_length])\n",
    "            next_char.append(self.text[index+seq_length])\n",
    "        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]"
   ]
  },
  {
   "source": [
    "### 定義模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, num_chars, batch_size, seq_length):\n",
    "        super().__init__()\n",
    "        self.num_chars = num_chars\n",
    "        self.seq_length = seq_length\n",
    "        self.batch_size = batch_size\n",
    "        self.cell = tf.keras.layers.LSTMCell(units=256)\n",
    "        self.dense = tf.keras.layers.Dense(units=self.num_chars)\n",
    "\n",
    "    def call(self, inputs, from_logits=False):\n",
    "        inputs = tf.one_hot(inputs, depth=self.num_chars)       # [batch_size, seq_length, num_chars]\n",
    "        state = self.cell.get_initial_state(batch_size=self.batch_size, dtype=tf.float32)\n",
    "\n",
    "        for t in range(self.seq_length):\n",
    "            output, state = self.cell(inputs[:, t, :], state)\n",
    "\n",
    "        logits = self.dense(output)\n",
    "\n",
    "        if from_logits:\n",
    "            return logits\n",
    "        else:\n",
    "            return tf.nn.softmax(logits)\n",
    "\n",
    "    def predict(self, inputs, temperature=1.):\n",
    "        batch_size, _ = tf.shape(inputs)\n",
    "        logits = self(inputs, from_logits=True)\n",
    "        prob = tf.nn.softmax(logits / temperature).numpy()\n",
    "        return np.array([np.random.choice(self.num_chars, p=prob[i, :])\n",
    "            for i in range(batch_size.numpy())])"
   ]
  },
  {
   "source": [
    "## 模型的訓練"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_batches = 100\n",
    "seq_length = 40\n",
    "batch_size = 50\n",
    "learning_rate = 1e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{0: '\\n', 1: ' ', 2: '!', 3: '\"', 4: \"'\", 5: '(', 6: ')', 7: ',', 8: '-', 9: '.', 10: '0', 11: '1', 12: '2', 13: '3', 14: '4', 15: '5', 16: '6', 17: '7', 18: '8', 19: '9', 20: ':', 21: ';', 22: '=', 23: '?', 24: '[', 25: ']', 26: '_', 27: 'a', 28: 'b', 29: 'c', 30: 'd', 31: 'e', 32: 'f', 33: 'g', 34: 'h', 35: 'i', 36: 'j', 37: 'k', 38: 'l', 39: 'm', 40: 'n', 41: 'o', 42: 'p', 43: 'q', 44: 'r', 45: 's', 46: 't', 47: 'u', 48: 'v', 49: 'w', 50: 'x', 51: 'y', 52: 'z', 53: 'ä', 54: 'æ', 55: 'é', 56: 'ë'}\n57\n"
     ]
    }
   ],
   "source": [
    "data_loader = DataLoader()\n",
    "\n",
    "model = RNN(num_chars=len(data_loader.chars), batch_size=batch_size, seq_length=seq_length)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(seq_length, batch_size)\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "source": [
    "## 模型的評估"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "diversity 0.200000:\n",
      " t t t   e t e  t o  ae t e ta thtat o aaaitt  atti t  t e  oe t tie  tte e t n ti  e ettete  t t r tiniait ee tat  tet  tit  aatte  it a t  etin t a oteat nie e tte tae stt tote tire i tit    t  nn t  o t eeaito   t ni   na ei te  teertttieeeeot  s  e  ta  t  e  eetan  t attaito aie et    e  t  oe e t tio att a tte a t t aiett  ee t tiot tttin i t  te ts t e as ttee  t  e t  et tte    a t  i  tti\n",
      "\n"
     ]
    }
   ],
   "source": [
    "X_, _ = data_loader.get_batch(seq_length, 1)\n",
    "for diversity in [0.2]:\n",
    "    X = X_\n",
    "    print(\"diversity %f:\" % diversity)\n",
    "    for t in range(400):\n",
    "        y_pred = model.predict(X, diversity)\n",
    "        print(data_loader.indices_char[y_pred[0]], end='', flush=True)\n",
    "        X = np.concatenate([X[:, 1:], np.expand_dims(y_pred, axis=1)], axis=-1)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python379jvsc74a57bd001ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f",
   "display_name": "Python 3.7.9  ('tensorflow2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "metadata": {
   "interpreter": {
    "hash": "01ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}