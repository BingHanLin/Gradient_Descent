{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])\n",
    "y = tf.constant([[10.0], [20.0]])\n",
    "\n",
    "class Linear(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.dense = tf.keras.layers.Dense(\n",
    "            units=1,\n",
    "            activation=None,\n",
    "            kernel_initializer=tf.zeros_initializer(),\n",
    "            bias_initializer=tf.zeros_initializer()\n",
    "        )\n",
    "\n",
    "    def call(self, input):\n",
    "        output = self.dense(input)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[<tf.Variable 'linear/dense/kernel:0' shape=(3, 1) dtype=float32, numpy=\narray([[0.40784496],\n       [1.191065  ],\n       [1.9742855 ]], dtype=float32)>, <tf.Variable 'linear/dense/bias:0' shape=(1,) dtype=float32, numpy=array([0.78322077], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "model = Linear()\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "for i in range(100):\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)      # 呼叫模型 y_pred = model(X) 而不是顯式寫出 y_pred = a * X + b\n",
    "        loss = tf.reduce_mean(tf.square(y_pred - y))\n",
    "    grads = tape.gradient(loss, model.variables)    # 使用 model.variables 這一屬性直接獲得模型中的所有變數\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))\n",
    "print(model.variables)"
   ]
  },
  {
   "source": [
    "## 多層感知器（MLP）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 載入資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的圖片預設為uint8（0-255的數字）。以下程式碼將其正規化到0-1之間的浮點數，並在最後增加一維作為顏色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機取出batch_size個元素並返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "source": [
    "### 定義模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = tf.keras.layers.Flatten()    # Flatten層將除第一維（batch_size）以外的維度展平\n",
    "        self.dense1 = tf.keras.layers.Dense(units=100, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):         # [batch_size, 28, 28, 1]\n",
    "        x = self.flatten(inputs)    # [batch_size, 784]\n",
    "        x = self.dense1(x)          # [batch_size, 100]\n",
    "        x = self.dense2(x)          # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "source": [
    "## 模型的訓練"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLP()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "35622\n",
      "batch 5231: loss 0.096071\n",
      "batch 5232: loss 0.032834\n",
      "batch 5233: loss 0.024130\n",
      "batch 5234: loss 0.006913\n",
      "batch 5235: loss 0.074678\n",
      "batch 5236: loss 0.169559\n",
      "batch 5237: loss 0.171786\n",
      "batch 5238: loss 0.019059\n",
      "batch 5239: loss 0.012958\n",
      "batch 5240: loss 0.126625\n",
      "batch 5241: loss 0.033680\n",
      "batch 5242: loss 0.048267\n",
      "batch 5243: loss 0.040503\n",
      "batch 5244: loss 0.003568\n",
      "batch 5245: loss 0.018695\n",
      "batch 5246: loss 0.067474\n",
      "batch 5247: loss 0.038873\n",
      "batch 5248: loss 0.038274\n",
      "batch 5249: loss 0.148936\n",
      "batch 5250: loss 0.027773\n",
      "batch 5251: loss 0.005688\n",
      "batch 5252: loss 0.006748\n",
      "batch 5253: loss 0.022442\n",
      "batch 5254: loss 0.011285\n",
      "batch 5255: loss 0.031777\n",
      "batch 5256: loss 0.011046\n",
      "batch 5257: loss 0.059389\n",
      "batch 5258: loss 0.008600\n",
      "batch 5259: loss 0.034845\n",
      "batch 5260: loss 0.055064\n",
      "batch 5261: loss 0.031936\n",
      "batch 5262: loss 0.039145\n",
      "batch 5263: loss 0.043539\n",
      "batch 5264: loss 0.022977\n",
      "batch 5265: loss 0.013986\n",
      "batch 5266: loss 0.034722\n",
      "batch 5267: loss 0.108650\n",
      "batch 5268: loss 0.060134\n",
      "batch 5269: loss 0.111821\n",
      "batch 5270: loss 0.042788\n",
      "batch 5271: loss 0.026857\n",
      "batch 5272: loss 0.056635\n",
      "batch 5273: loss 0.086071\n",
      "batch 5274: loss 0.085681\n",
      "batch 5275: loss 0.007077\n",
      "batch 5276: loss 0.027356\n",
      "batch 5277: loss 0.032786\n",
      "batch 5278: loss 0.166917\n",
      "batch 5279: loss 0.048723\n",
      "batch 5280: loss 0.023082\n",
      "batch 5281: loss 0.061699\n",
      "batch 5282: loss 0.066204\n",
      "batch 5283: loss 0.055733\n",
      "batch 5284: loss 0.097526\n",
      "batch 5285: loss 0.006189\n",
      "batch 5286: loss 0.015575\n",
      "batch 5287: loss 0.079720\n",
      "batch 5288: loss 0.059439\n",
      "batch 5289: loss 0.037814\n",
      "batch 5290: loss 0.027262\n",
      "batch 5291: loss 0.026374\n",
      "batch 5292: loss 0.046132\n",
      "batch 5293: loss 0.038881\n",
      "batch 5294: loss 0.013136\n",
      "batch 5295: loss 0.055714\n",
      "batch 5296: loss 0.023299\n",
      "batch 5297: loss 0.061788\n",
      "batch 5298: loss 0.091770\n",
      "batch 5299: loss 0.020512\n",
      "batch 5300: loss 0.017128\n",
      "batch 5301: loss 0.050060\n",
      "batch 5302: loss 0.050852\n",
      "batch 5303: loss 0.054418\n",
      "batch 5304: loss 0.009213\n",
      "batch 5305: loss 0.160785\n",
      "batch 5306: loss 0.079964\n",
      "batch 5307: loss 0.009301\n",
      "batch 5308: loss 0.237309\n",
      "batch 5309: loss 0.125322\n",
      "batch 5310: loss 0.022687\n",
      "batch 5311: loss 0.030481\n",
      "batch 5312: loss 0.097502\n",
      "batch 5313: loss 0.029797\n",
      "batch 5314: loss 0.040912\n",
      "batch 5315: loss 0.049918\n",
      "batch 5316: loss 0.018664\n",
      "batch 5317: loss 0.059478\n",
      "batch 5318: loss 0.025917\n",
      "batch 5319: loss 0.003260\n",
      "batch 5320: loss 0.038314\n",
      "batch 5321: loss 0.019500\n",
      "batch 5322: loss 0.050771\n",
      "batch 5323: loss 0.009739\n",
      "batch 5324: loss 0.035395\n",
      "batch 5325: loss 0.050636\n",
      "batch 5326: loss 0.073599\n",
      "batch 5327: loss 0.056107\n",
      "batch 5328: loss 0.017397\n",
      "batch 5329: loss 0.005396\n",
      "batch 5330: loss 0.048924\n",
      "batch 5331: loss 0.008853\n",
      "batch 5332: loss 0.009243\n",
      "batch 5333: loss 0.020531\n",
      "batch 5334: loss 0.207459\n",
      "batch 5335: loss 0.106188\n",
      "batch 5336: loss 0.111593\n",
      "batch 5337: loss 0.074645\n",
      "batch 5338: loss 0.056315\n",
      "batch 5339: loss 0.043805\n",
      "batch 5340: loss 0.023056\n",
      "batch 5341: loss 0.044530\n",
      "batch 5342: loss 0.027248\n",
      "batch 5343: loss 0.129711\n",
      "batch 5344: loss 0.178983\n",
      "batch 5345: loss 0.014305\n",
      "batch 5346: loss 0.044887\n",
      "batch 5347: loss 0.025213\n",
      "batch 5348: loss 0.045085\n",
      "batch 5349: loss 0.029234\n",
      "batch 5350: loss 0.019796\n",
      "batch 5351: loss 0.015811\n",
      "batch 5352: loss 0.063326\n",
      "batch 5353: loss 0.055471\n",
      "batch 5354: loss 0.105665\n",
      "batch 5355: loss 0.014452\n",
      "batch 5356: loss 0.010539\n",
      "batch 5357: loss 0.117034\n",
      "batch 5358: loss 0.007942\n",
      "batch 5359: loss 0.023322\n",
      "batch 5360: loss 0.032798\n",
      "batch 5361: loss 0.103286\n",
      "batch 5362: loss 0.010812\n",
      "batch 5363: loss 0.052903\n",
      "batch 5364: loss 0.064928\n",
      "batch 5365: loss 0.024409\n",
      "batch 5366: loss 0.040041\n",
      "batch 5367: loss 0.043653\n",
      "batch 5368: loss 0.036160\n",
      "batch 5369: loss 0.009591\n",
      "batch 5370: loss 0.090230\n",
      "batch 5371: loss 0.065654\n",
      "batch 5372: loss 0.024383\n",
      "batch 5373: loss 0.107020\n",
      "batch 5374: loss 0.027192\n",
      "batch 5375: loss 0.045711\n",
      "batch 5376: loss 0.010773\n",
      "batch 5377: loss 0.072075\n",
      "batch 5378: loss 0.007242\n",
      "batch 5379: loss 0.047450\n",
      "batch 5380: loss 0.011441\n",
      "batch 5381: loss 0.013676\n",
      "batch 5382: loss 0.006970\n",
      "batch 5383: loss 0.011784\n",
      "batch 5384: loss 0.021628\n",
      "batch 5385: loss 0.044574\n",
      "batch 5386: loss 0.014801\n",
      "batch 5387: loss 0.041748\n",
      "batch 5388: loss 0.063509\n",
      "batch 5389: loss 0.155567\n",
      "batch 5390: loss 0.023423\n",
      "batch 5391: loss 0.048621\n",
      "batch 5392: loss 0.367193\n",
      "batch 5393: loss 0.008724\n",
      "batch 5394: loss 0.018261\n",
      "batch 5395: loss 0.055080\n",
      "batch 5396: loss 0.051996\n",
      "batch 5397: loss 0.049529\n",
      "batch 5398: loss 0.170668\n",
      "batch 5399: loss 0.016928\n",
      "batch 5400: loss 0.023545\n",
      "batch 5401: loss 0.068971\n",
      "batch 5402: loss 0.049495\n",
      "batch 5403: loss 0.035812\n",
      "batch 5404: loss 0.070773\n",
      "batch 5405: loss 0.012088\n",
      "batch 5406: loss 0.032368\n",
      "batch 5407: loss 0.071276\n",
      "batch 5408: loss 0.228834\n",
      "batch 5409: loss 0.062722\n",
      "batch 5410: loss 0.021531\n",
      "batch 5411: loss 0.085675\n",
      "batch 5412: loss 0.080143\n",
      "batch 5413: loss 0.131890\n",
      "batch 5414: loss 0.005727\n",
      "batch 5415: loss 0.003277\n",
      "batch 5416: loss 0.037219\n",
      "batch 5417: loss 0.032997\n",
      "batch 5418: loss 0.035682\n",
      "batch 5419: loss 0.018345\n",
      "batch 5420: loss 0.018346\n",
      "batch 5421: loss 0.012090\n",
      "batch 5422: loss 0.027028\n",
      "batch 5423: loss 0.051449\n",
      "batch 5424: loss 0.030485\n",
      "batch 5425: loss 0.019459\n",
      "batch 5426: loss 0.169684\n",
      "batch 5427: loss 0.030010\n",
      "batch 5428: loss 0.017161\n",
      "batch 5429: loss 0.082122\n",
      "batch 5430: loss 0.114633\n",
      "batch 5431: loss 0.028182\n",
      "batch 5432: loss 0.002499\n",
      "batch 5433: loss 0.017849\n",
      "batch 5434: loss 0.004138\n",
      "batch 5435: loss 0.077437\n",
      "batch 5436: loss 0.027536\n",
      "batch 5437: loss 0.028641\n",
      "batch 5438: loss 0.009653\n",
      "batch 5439: loss 0.034789\n",
      "batch 5440: loss 0.071539\n",
      "batch 5441: loss 0.018385\n",
      "batch 5442: loss 0.045511\n",
      "batch 5443: loss 0.038710\n",
      "batch 5444: loss 0.156340\n",
      "batch 5445: loss 0.128363\n",
      "batch 5446: loss 0.021354\n",
      "batch 5447: loss 0.076250\n",
      "batch 5448: loss 0.021794\n",
      "batch 5449: loss 0.156992\n",
      "batch 5450: loss 0.061837\n",
      "batch 5451: loss 0.011126\n",
      "batch 5452: loss 0.021800\n",
      "batch 5453: loss 0.013844\n",
      "batch 5454: loss 0.012080\n",
      "batch 5455: loss 0.030150\n",
      "batch 5456: loss 0.147652\n",
      "batch 5457: loss 0.028762\n",
      "batch 5458: loss 0.202838\n",
      "batch 5459: loss 0.027226\n",
      "batch 5460: loss 0.016567\n",
      "batch 5461: loss 0.013709\n",
      "batch 5462: loss 0.016233\n",
      "batch 5463: loss 0.037440\n",
      "batch 5464: loss 0.029430\n",
      "batch 5465: loss 0.087428\n",
      "batch 5466: loss 0.054487\n",
      "batch 5467: loss 0.119649\n",
      "batch 5468: loss 0.065524\n",
      "batch 5469: loss 0.022522\n",
      "batch 5470: loss 0.046730\n",
      "batch 5471: loss 0.023227\n",
      "batch 5472: loss 0.075586\n",
      "batch 5473: loss 0.025049\n",
      "batch 5474: loss 0.104006\n",
      "batch 5475: loss 0.010014\n",
      "batch 5476: loss 0.087702\n",
      "batch 5477: loss 0.019311\n",
      "batch 5478: loss 0.023056\n",
      "batch 5479: loss 0.040947\n",
      "batch 5480: loss 0.102673\n",
      "batch 5481: loss 0.013910\n",
      "batch 5482: loss 0.008716\n",
      "batch 5483: loss 0.160852\n",
      "batch 5484: loss 0.018926\n",
      "batch 5485: loss 0.063959\n",
      "batch 5486: loss 0.095725\n",
      "batch 5487: loss 0.010697\n",
      "batch 5488: loss 0.024137\n",
      "batch 5489: loss 0.040019\n",
      "batch 5490: loss 0.015633\n",
      "batch 5491: loss 0.053312\n",
      "batch 5492: loss 0.084996\n",
      "batch 5493: loss 0.109988\n",
      "batch 5494: loss 0.102235\n",
      "batch 5495: loss 0.042439\n",
      "batch 5496: loss 0.025309\n",
      "batch 5497: loss 0.156490\n",
      "batch 5498: loss 0.027133\n",
      "batch 5499: loss 0.012614\n",
      "batch 5500: loss 0.060193\n",
      "batch 5501: loss 0.088618\n",
      "batch 5502: loss 0.030403\n",
      "batch 5503: loss 0.042691\n",
      "batch 5504: loss 0.007950\n",
      "batch 5505: loss 0.049624\n",
      "batch 5506: loss 0.037540\n",
      "batch 5507: loss 0.058484\n",
      "batch 5508: loss 0.019566\n",
      "batch 5509: loss 0.180099\n",
      "batch 5510: loss 0.042468\n",
      "batch 5511: loss 0.225268\n",
      "batch 5512: loss 0.084587\n",
      "batch 5513: loss 0.023713\n",
      "batch 5514: loss 0.008915\n",
      "batch 5515: loss 0.095051\n",
      "batch 5516: loss 0.026614\n",
      "batch 5517: loss 0.154895\n",
      "batch 5518: loss 0.067725\n",
      "batch 5519: loss 0.073991\n",
      "batch 5520: loss 0.126062\n",
      "batch 5521: loss 0.028939\n",
      "batch 5522: loss 0.063118\n",
      "batch 5523: loss 0.029774\n",
      "batch 5524: loss 0.157735\n",
      "batch 5525: loss 0.015797\n",
      "batch 5526: loss 0.037022\n",
      "batch 5527: loss 0.025849\n",
      "batch 5528: loss 0.029327\n",
      "batch 5529: loss 0.012214\n",
      "batch 5530: loss 0.048269\n",
      "batch 5531: loss 0.133564\n",
      "batch 5532: loss 0.014812\n",
      "batch 5533: loss 0.035856\n",
      "batch 5534: loss 0.032405\n",
      "batch 5535: loss 0.058821\n",
      "batch 5536: loss 0.074824\n",
      "batch 5537: loss 0.026725\n",
      "batch 5538: loss 0.085971\n",
      "batch 5539: loss 0.094094\n",
      "batch 5540: loss 0.018807\n",
      "batch 5541: loss 0.022148\n",
      "batch 5542: loss 0.019388\n",
      "batch 5543: loss 0.013893\n",
      "batch 5544: loss 0.017769\n",
      "batch 5545: loss 0.013955\n",
      "batch 5546: loss 0.043235\n",
      "batch 5547: loss 0.019541\n",
      "batch 5548: loss 0.043589\n",
      "batch 5549: loss 0.049109\n",
      "batch 5550: loss 0.009310\n",
      "batch 5551: loss 0.033565\n",
      "batch 5552: loss 0.003449\n",
      "batch 5553: loss 0.127152\n",
      "batch 5554: loss 0.020571\n",
      "batch 5555: loss 0.088562\n",
      "batch 5556: loss 0.160533\n",
      "batch 5557: loss 0.020410\n",
      "batch 5558: loss 0.036098\n",
      "batch 5559: loss 0.034866\n",
      "batch 5560: loss 0.020683\n",
      "batch 5561: loss 0.129056\n",
      "batch 5562: loss 0.023511\n",
      "batch 5563: loss 0.093437\n",
      "batch 5564: loss 0.017935\n",
      "batch 5565: loss 0.052367\n",
      "batch 5566: loss 0.047075\n",
      "batch 5567: loss 0.017263\n",
      "batch 5568: loss 0.062108\n",
      "batch 5569: loss 0.020265\n",
      "batch 5570: loss 0.076152\n",
      "batch 5571: loss 0.029854\n",
      "batch 5572: loss 0.007551\n",
      "batch 5573: loss 0.055291\n",
      "batch 5574: loss 0.059688\n",
      "batch 5575: loss 0.001732\n",
      "batch 5576: loss 0.013571\n",
      "batch 5577: loss 0.122758\n",
      "batch 5578: loss 0.287808\n",
      "batch 5579: loss 0.074138\n",
      "batch 5580: loss 0.031790\n",
      "batch 5581: loss 0.111932\n",
      "batch 5582: loss 0.013753\n",
      "batch 5583: loss 0.010737\n",
      "batch 5584: loss 0.041243\n",
      "batch 5585: loss 0.006765\n",
      "batch 5586: loss 0.035038\n",
      "batch 5587: loss 0.036675\n",
      "batch 5588: loss 0.200401\n",
      "batch 5589: loss 0.024113\n",
      "batch 5590: loss 0.023947\n",
      "batch 5591: loss 0.082196\n",
      "batch 5592: loss 0.011426\n",
      "batch 5593: loss 0.034879\n",
      "batch 5594: loss 0.068504\n",
      "batch 5595: loss 0.012746\n",
      "batch 5596: loss 0.017712\n",
      "batch 5597: loss 0.134962\n",
      "batch 5598: loss 0.024379\n",
      "batch 5599: loss 0.036170\n",
      "batch 5600: loss 0.005664\n",
      "batch 5601: loss 0.008489\n",
      "batch 5602: loss 0.031201\n",
      "batch 5603: loss 0.059115\n",
      "batch 5604: loss 0.012282\n",
      "batch 5605: loss 0.049151\n",
      "batch 5606: loss 0.021379\n",
      "batch 5607: loss 0.011608\n",
      "batch 5608: loss 0.126310\n",
      "batch 5609: loss 0.019005\n",
      "batch 5610: loss 0.049319\n",
      "batch 5611: loss 0.042454\n",
      "batch 5612: loss 0.022856\n",
      "batch 5613: loss 0.055123\n",
      "batch 5614: loss 0.049346\n",
      "batch 5615: loss 0.037084\n",
      "batch 5616: loss 0.014203\n",
      "batch 5617: loss 0.054476\n",
      "batch 5618: loss 0.086887\n",
      "batch 5619: loss 0.022602\n",
      "batch 5620: loss 0.041583\n",
      "batch 5621: loss 0.058374\n",
      "batch 5622: loss 0.020013\n",
      "batch 5623: loss 0.074535\n",
      "batch 5624: loss 0.013585\n",
      "batch 5625: loss 0.048558\n",
      "batch 5626: loss 0.035884\n",
      "batch 5627: loss 0.157596\n",
      "batch 5628: loss 0.010461\n",
      "batch 5629: loss 0.072184\n",
      "batch 5630: loss 0.087557\n",
      "batch 5631: loss 0.005813\n",
      "batch 5632: loss 0.008638\n",
      "batch 5633: loss 0.064857\n",
      "batch 5634: loss 0.206129\n",
      "batch 5635: loss 0.023832\n",
      "batch 5636: loss 0.035413\n",
      "batch 5637: loss 0.005732\n",
      "batch 5638: loss 0.030028\n",
      "batch 5639: loss 0.014403\n",
      "batch 5640: loss 0.059112\n",
      "batch 5641: loss 0.072394\n",
      "batch 5642: loss 0.040637\n",
      "batch 5643: loss 0.144840\n",
      "batch 5644: loss 0.086261\n",
      "batch 5645: loss 0.022715\n",
      "batch 5646: loss 0.030143\n",
      "batch 5647: loss 0.055461\n",
      "batch 5648: loss 0.047787\n",
      "batch 5649: loss 0.082071\n",
      "batch 5650: loss 0.117542\n",
      "batch 5651: loss 0.011440\n",
      "batch 5652: loss 0.032703\n",
      "batch 5653: loss 0.136639\n",
      "batch 5654: loss 0.012393\n",
      "batch 5655: loss 0.040234\n",
      "batch 5656: loss 0.042680\n",
      "batch 5657: loss 0.108838\n",
      "batch 5658: loss 0.069323\n",
      "batch 5659: loss 0.010638\n",
      "batch 5660: loss 0.022140\n",
      "batch 5661: loss 0.030072\n",
      "batch 5662: loss 0.049932\n",
      "batch 5663: loss 0.040836\n",
      "batch 5664: loss 0.010383\n",
      "batch 5665: loss 0.108528\n",
      "batch 5666: loss 0.071789\n",
      "batch 5667: loss 0.075202\n",
      "batch 5668: loss 0.044758\n",
      "batch 5669: loss 0.034465\n",
      "batch 5670: loss 0.117811\n",
      "batch 5671: loss 0.010074\n",
      "batch 5672: loss 0.036774\n",
      "batch 5673: loss 0.138512\n",
      "batch 5674: loss 0.033779\n",
      "batch 5675: loss 0.045416\n",
      "batch 5676: loss 0.127433\n",
      "batch 5677: loss 0.073963\n",
      "batch 5678: loss 0.034816\n",
      "batch 5679: loss 0.021312\n",
      "batch 5680: loss 0.008353\n",
      "batch 5681: loss 0.104058\n",
      "batch 5682: loss 0.016585\n",
      "batch 5683: loss 0.007437\n",
      "batch 5684: loss 0.040776\n",
      "batch 5685: loss 0.014284\n",
      "batch 5686: loss 0.041969\n",
      "batch 5687: loss 0.024736\n",
      "batch 5688: loss 0.013647\n",
      "batch 5689: loss 0.046238\n",
      "batch 5690: loss 0.011880\n",
      "batch 5691: loss 0.153250\n",
      "batch 5692: loss 0.018983\n",
      "batch 5693: loss 0.026896\n",
      "batch 5694: loss 0.041530\n",
      "batch 5695: loss 0.101084\n",
      "batch 5696: loss 0.024828\n",
      "batch 5697: loss 0.022464\n",
      "batch 5698: loss 0.118163\n",
      "batch 5699: loss 0.170719\n",
      "batch 5700: loss 0.041605\n",
      "batch 5701: loss 0.010780\n",
      "batch 5702: loss 0.009394\n",
      "batch 5703: loss 0.010052\n",
      "batch 5704: loss 0.046702\n",
      "batch 5705: loss 0.174178\n",
      "batch 5706: loss 0.092893\n",
      "batch 5707: loss 0.085483\n",
      "batch 5708: loss 0.013439\n",
      "batch 5709: loss 0.105788\n",
      "batch 5710: loss 0.047200\n",
      "batch 5711: loss 0.061563\n",
      "batch 5712: loss 0.271988\n",
      "batch 5713: loss 0.005771\n",
      "batch 5714: loss 0.093078\n",
      "batch 5715: loss 0.074154\n",
      "batch 5716: loss 0.033042\n",
      "batch 5717: loss 0.022460\n",
      "batch 5718: loss 0.037141\n",
      "batch 5719: loss 0.070143\n",
      "batch 5720: loss 0.146214\n",
      "batch 5721: loss 0.090650\n",
      "batch 5722: loss 0.125475\n",
      "batch 5723: loss 0.032653\n",
      "batch 5724: loss 0.036437\n",
      "batch 5725: loss 0.032816\n",
      "batch 5726: loss 0.028480\n",
      "batch 5727: loss 0.026119\n",
      "batch 5728: loss 0.012343\n",
      "batch 5729: loss 0.009360\n",
      "batch 5730: loss 0.090492\n",
      "batch 5731: loss 0.063687\n",
      "batch 5732: loss 0.042575\n",
      "batch 5733: loss 0.153156\n",
      "batch 5734: loss 0.105898\n",
      "batch 5735: loss 0.017232\n",
      "batch 5736: loss 0.025655\n",
      "batch 5737: loss 0.117076\n",
      "batch 5738: loss 0.039154\n",
      "batch 5739: loss 0.030288\n",
      "batch 5740: loss 0.059966\n",
      "batch 5741: loss 0.020481\n",
      "batch 5742: loss 0.010647\n",
      "batch 5743: loss 0.075683\n",
      "batch 5744: loss 0.105549\n",
      "batch 5745: loss 0.010357\n",
      "batch 5746: loss 0.020662\n",
      "batch 5747: loss 0.035988\n",
      "batch 5748: loss 0.008104\n",
      "batch 5749: loss 0.009917\n",
      "batch 5750: loss 0.007213\n",
      "batch 5751: loss 0.038743\n",
      "batch 5752: loss 0.012027\n",
      "batch 5753: loss 0.017772\n",
      "batch 5754: loss 0.018721\n",
      "batch 5755: loss 0.021267\n",
      "batch 5756: loss 0.396964\n",
      "batch 5757: loss 0.036348\n",
      "batch 5758: loss 0.009252\n",
      "batch 5759: loss 0.028821\n",
      "batch 5760: loss 0.081588\n",
      "batch 5761: loss 0.013581\n",
      "batch 5762: loss 0.084801\n",
      "batch 5763: loss 0.011903\n",
      "batch 5764: loss 0.017471\n",
      "batch 5765: loss 0.087776\n",
      "batch 5766: loss 0.017881\n",
      "batch 5767: loss 0.017774\n",
      "batch 5768: loss 0.028566\n",
      "batch 5769: loss 0.015681\n",
      "batch 5770: loss 0.040417\n",
      "batch 5771: loss 0.142153\n",
      "batch 5772: loss 0.141315\n",
      "batch 5773: loss 0.011791\n",
      "batch 5774: loss 0.079699\n",
      "batch 5775: loss 0.079731\n",
      "batch 5776: loss 0.084210\n",
      "batch 5777: loss 0.057137\n",
      "batch 5778: loss 0.084839\n",
      "batch 5779: loss 0.020800\n",
      "batch 5780: loss 0.020856\n",
      "batch 5781: loss 0.049823\n",
      "batch 5782: loss 0.046028\n",
      "batch 5783: loss 0.032698\n",
      "batch 5784: loss 0.017054\n",
      "batch 5785: loss 0.009556\n",
      "batch 5786: loss 0.029032\n",
      "batch 5787: loss 0.023439\n",
      "batch 5788: loss 0.031618\n",
      "batch 5789: loss 0.016103\n",
      "batch 5790: loss 0.010450\n",
      "batch 5791: loss 0.020649\n",
      "batch 5792: loss 0.021520\n",
      "batch 5793: loss 0.045610\n",
      "batch 5794: loss 0.046194\n",
      "batch 5795: loss 0.012501\n",
      "batch 5796: loss 0.053376\n",
      "batch 5797: loss 0.129393\n",
      "batch 5798: loss 0.055758\n",
      "batch 5799: loss 0.066986\n",
      "batch 5800: loss 0.063565\n",
      "batch 5801: loss 0.029940\n",
      "batch 5802: loss 0.042902\n",
      "batch 5803: loss 0.089786\n",
      "batch 5804: loss 0.008765\n",
      "batch 5805: loss 0.034979\n",
      "batch 5806: loss 0.073433\n",
      "batch 5807: loss 0.063178\n",
      "batch 5808: loss 0.013592\n",
      "batch 5809: loss 0.010204\n",
      "batch 5810: loss 0.018363\n",
      "batch 5811: loss 0.059524\n",
      "batch 5812: loss 0.052202\n",
      "batch 5813: loss 0.008485\n",
      "batch 5814: loss 0.023097\n",
      "batch 5815: loss 0.050051\n",
      "batch 5816: loss 0.074745\n",
      "batch 5817: loss 0.163098\n",
      "batch 5818: loss 0.016489\n",
      "batch 5819: loss 0.013155\n",
      "batch 5820: loss 0.027962\n",
      "batch 5821: loss 0.026473\n",
      "batch 5822: loss 0.148407\n",
      "batch 5823: loss 0.166259\n",
      "batch 5824: loss 0.073495\n",
      "batch 5825: loss 0.103260\n",
      "batch 5826: loss 0.039332\n",
      "batch 5827: loss 0.036424\n",
      "batch 5828: loss 0.055338\n",
      "batch 5829: loss 0.018671\n",
      "batch 5830: loss 0.049744\n",
      "batch 5831: loss 0.021993\n",
      "batch 5832: loss 0.096674\n",
      "batch 5833: loss 0.014964\n",
      "batch 5834: loss 0.024783\n",
      "batch 5835: loss 0.033825\n",
      "batch 5836: loss 0.011998\n",
      "batch 5837: loss 0.023045\n",
      "batch 5838: loss 0.083370\n",
      "batch 5839: loss 0.035921\n",
      "batch 5840: loss 0.066928\n",
      "batch 5841: loss 0.066296\n",
      "batch 5842: loss 0.020865\n",
      "batch 5843: loss 0.049206\n",
      "batch 5844: loss 0.122473\n",
      "batch 5845: loss 0.039992\n",
      "batch 5846: loss 0.082989\n",
      "batch 5847: loss 0.010677\n",
      "batch 5848: loss 0.008778\n",
      "batch 5849: loss 0.031847\n",
      "batch 5850: loss 0.022868\n",
      "batch 5851: loss 0.048589\n",
      "batch 5852: loss 0.036193\n",
      "batch 5853: loss 0.064664\n",
      "batch 5854: loss 0.028004\n",
      "batch 5855: loss 0.126606\n",
      "batch 5856: loss 0.069304\n",
      "batch 5857: loss 0.016377\n",
      "batch 5858: loss 0.078436\n",
      "batch 5859: loss 0.130438\n",
      "batch 5860: loss 0.067029\n",
      "batch 5861: loss 0.071216\n",
      "batch 5862: loss 0.013365\n",
      "batch 5863: loss 0.021126\n",
      "batch 5864: loss 0.027233\n",
      "batch 5865: loss 0.042620\n",
      "batch 5866: loss 0.025245\n",
      "batch 5867: loss 0.030469\n",
      "batch 5868: loss 0.013438\n",
      "batch 5869: loss 0.024752\n",
      "batch 5870: loss 0.095529\n",
      "batch 5871: loss 0.125908\n",
      "batch 5872: loss 0.035101\n",
      "batch 5873: loss 0.026114\n",
      "batch 5874: loss 0.098434\n",
      "batch 5875: loss 0.111813\n",
      "batch 5876: loss 0.031500\n",
      "batch 5877: loss 0.018421\n",
      "batch 5878: loss 0.071879\n",
      "batch 5879: loss 0.054894\n",
      "batch 5880: loss 0.109958\n",
      "batch 5881: loss 0.061976\n",
      "batch 5882: loss 0.013252\n",
      "batch 5883: loss 0.035317\n",
      "batch 5884: loss 0.076511\n",
      "batch 5885: loss 0.012072\n",
      "batch 5886: loss 0.277434\n",
      "batch 5887: loss 0.022143\n",
      "batch 5888: loss 0.010589\n",
      "batch 5889: loss 0.108159\n",
      "batch 5890: loss 0.282839\n",
      "batch 5891: loss 0.074689\n",
      "batch 5892: loss 0.126590\n",
      "batch 5893: loss 0.030950\n",
      "batch 5894: loss 0.006477\n",
      "batch 5895: loss 0.217338\n",
      "batch 5896: loss 0.055574\n",
      "batch 5897: loss 0.044923\n",
      "batch 5898: loss 0.039815\n",
      "batch 5899: loss 0.049472\n",
      "batch 5900: loss 0.010732\n",
      "batch 5901: loss 0.042826\n",
      "batch 5902: loss 0.010563\n",
      "batch 5903: loss 0.142737\n",
      "batch 5904: loss 0.043058\n",
      "batch 5905: loss 0.016743\n",
      "batch 5906: loss 0.061819\n",
      "batch 5907: loss 0.036968\n",
      "batch 5908: loss 0.070781\n",
      "batch 5909: loss 0.066941\n",
      "batch 5910: loss 0.028109\n",
      "batch 5911: loss 0.056118\n",
      "batch 5912: loss 0.009606\n",
      "batch 5913: loss 0.030891\n",
      "batch 5914: loss 0.014652\n",
      "batch 5915: loss 0.015839\n",
      "batch 5916: loss 0.097782\n",
      "batch 5917: loss 0.043885\n",
      "batch 5918: loss 0.059149\n",
      "batch 5919: loss 0.053404\n",
      "batch 5920: loss 0.016869\n",
      "batch 5921: loss 0.021622\n",
      "batch 5922: loss 0.044951\n",
      "batch 5923: loss 0.062688\n",
      "batch 5924: loss 0.057192\n",
      "batch 5925: loss 0.072432\n",
      "batch 5926: loss 0.014201\n",
      "batch 5927: loss 0.010465\n",
      "batch 5928: loss 0.014201\n",
      "batch 5929: loss 0.166932\n",
      "batch 5930: loss 0.055278\n",
      "batch 5931: loss 0.016767\n",
      "batch 5932: loss 0.108618\n",
      "batch 5933: loss 0.014266\n",
      "batch 5934: loss 0.067350\n",
      "batch 5935: loss 0.023611\n",
      "batch 5936: loss 0.058205\n",
      "batch 5937: loss 0.036210\n",
      "batch 5938: loss 0.058266\n",
      "batch 5939: loss 0.093605\n",
      "batch 5940: loss 0.021599\n",
      "batch 5941: loss 0.005724\n",
      "batch 5942: loss 0.044525\n",
      "batch 5943: loss 0.024815\n",
      "batch 5944: loss 0.007646\n",
      "batch 5945: loss 0.087156\n",
      "batch 5946: loss 0.043975\n",
      "batch 5947: loss 0.026570\n",
      "batch 5948: loss 0.005866\n",
      "batch 5949: loss 0.289930\n",
      "batch 5950: loss 0.046733\n",
      "batch 5951: loss 0.057676\n",
      "batch 5952: loss 0.016627\n",
      "batch 5953: loss 0.022581\n",
      "batch 5954: loss 0.032572\n",
      "batch 5955: loss 0.073468\n",
      "batch 5956: loss 0.035104\n",
      "batch 5957: loss 0.073701\n",
      "batch 5958: loss 0.108973\n",
      "batch 5959: loss 0.059335\n",
      "batch 5960: loss 0.041016\n",
      "batch 5961: loss 0.055266\n",
      "batch 5962: loss 0.039746\n",
      "batch 5963: loss 0.089169\n",
      "batch 5964: loss 0.070039\n",
      "batch 5965: loss 0.012090\n",
      "batch 5966: loss 0.015780\n",
      "batch 5967: loss 0.036285\n",
      "batch 5968: loss 0.019522\n",
      "batch 5969: loss 0.035795\n",
      "batch 5970: loss 0.050958\n",
      "batch 5971: loss 0.043290\n",
      "batch 5972: loss 0.034277\n",
      "batch 5973: loss 0.036845\n",
      "batch 5974: loss 0.165527\n",
      "batch 5975: loss 0.037912\n",
      "batch 5976: loss 0.009127\n",
      "batch 5977: loss 0.052534\n",
      "batch 5978: loss 0.021200\n",
      "batch 5979: loss 0.055887\n",
      "batch 5980: loss 0.225023\n",
      "batch 5981: loss 0.025969\n",
      "batch 5982: loss 0.031865\n",
      "batch 5983: loss 0.015340\n",
      "batch 5984: loss 0.095549\n",
      "batch 5985: loss 0.023279\n",
      "batch 5986: loss 0.105466\n",
      "batch 5987: loss 0.020113\n",
      "batch 5988: loss 0.114689\n",
      "batch 5989: loss 0.034456\n",
      "batch 5990: loss 0.045610\n",
      "batch 5991: loss 0.013232\n",
      "batch 5992: loss 0.020912\n",
      "batch 5993: loss 0.014810\n",
      "batch 5994: loss 0.109190\n",
      "batch 5995: loss 0.008320\n",
      "batch 5996: loss 0.028898\n",
      "batch 5997: loss 0.012711\n",
      "batch 5998: loss 0.043121\n",
      "batch 5999: loss 0.059416\n"
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: loss %f\" % (batch_index, loss.numpy()))\n",
    "\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "source": [
    "## 模型的評估"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test accuracy: 0.974800\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "    \n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd001ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f",
   "display_name": "Python 3.7.5  ('tensorflow2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "01ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}