{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow and tf.keras\n",
    "import tensorflow as tf\n",
    "\n",
    "# Helper libraries\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "source": [
    "## 卷積神經網路（CNN）"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "### 載入資料"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MNISTLoader():\n",
    "    def __init__(self):\n",
    "        mnist = tf.keras.datasets.mnist\n",
    "        (self.train_data, self.train_label), (self.test_data, self.test_label) = mnist.load_data()\n",
    "        # MNIST中的圖片預設為uint8（0-255的數字）。以下程式碼將其正規化到0-1之間的浮點數，並在最後增加一維作為顏色通道\n",
    "        self.train_data = np.expand_dims(self.train_data.astype(np.float32) / 255.0, axis=-1)      # [60000, 28, 28, 1]\n",
    "        self.test_data = np.expand_dims(self.test_data.astype(np.float32) / 255.0, axis=-1)        # [10000, 28, 28, 1]\n",
    "        self.train_label = self.train_label.astype(np.int32)    # [60000]\n",
    "        self.test_label = self.test_label.astype(np.int32)      # [10000]\n",
    "        self.num_train_data, self.num_test_data = self.train_data.shape[0], self.test_data.shape[0]\n",
    "\n",
    "    def get_batch(self, batch_size):\n",
    "        # 從資料集中隨機取出batch_size個元素並返回\n",
    "        index = np.random.randint(0, self.num_train_data, batch_size)\n",
    "        return self.train_data[index, :], self.train_label[index]"
   ]
  },
  {
   "source": [
    "### 定義模型"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = tf.keras.layers.Conv2D(\n",
    "            filters=32,             # 卷積層神經元（卷積核）數目\n",
    "            kernel_size=[5, 5],     # 接受區的大小\n",
    "            padding='same',         # padding策略（vaild 或 same）\n",
    "            activation=tf.nn.relu   # 激活函数\n",
    "        )\n",
    "        self.pool1 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.conv2 = tf.keras.layers.Conv2D(\n",
    "            filters=64,\n",
    "            kernel_size=[5, 5],\n",
    "            padding='same',\n",
    "            activation=tf.nn.relu\n",
    "        )\n",
    "        self.pool2 = tf.keras.layers.MaxPool2D(pool_size=[2, 2], strides=2)\n",
    "        self.flatten = tf.keras.layers.Reshape(target_shape=(7 * 7 * 64,))\n",
    "        self.dense1 = tf.keras.layers.Dense(units=1024, activation=tf.nn.relu)\n",
    "        self.dense2 = tf.keras.layers.Dense(units=10)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        x = self.conv1(inputs)                  # [batch_size, 28, 28, 32]\n",
    "        x = self.pool1(x)                       # [batch_size, 14, 14, 32]\n",
    "        x = self.conv2(x)                       # [batch_size, 14, 14, 64]\n",
    "        x = self.pool2(x)                       # [batch_size, 7, 7, 64]\n",
    "        x = self.flatten(x)                     # [batch_size, 7 * 7 * 64]\n",
    "        x = self.dense1(x)                      # [batch_size, 1024]\n",
    "        x = self.dense2(x)                      # [batch_size, 10]\n",
    "        output = tf.nn.softmax(x)\n",
    "        return output"
   ]
  },
  {
   "source": [
    "## 模型的訓練"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "batch_size = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CNN()\n",
    "data_loader = MNISTLoader()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "177: ,loss 0.000350\n",
      "batch 5178: ,loss 0.001316\n",
      "batch 5179: ,loss 0.000199\n",
      "batch 5180: ,loss 0.000825\n",
      "batch 5181: ,loss 0.000044\n",
      "batch 5182: ,loss 0.000152\n",
      "batch 5183: ,loss 0.004099\n",
      "batch 5184: ,loss 0.006514\n",
      "batch 5185: ,loss 0.000116\n",
      "batch 5186: ,loss 0.006989\n",
      "batch 5187: ,loss 0.062304\n",
      "batch 5188: ,loss 0.046250\n",
      "batch 5189: ,loss 0.000386\n",
      "batch 5190: ,loss 0.001184\n",
      "batch 5191: ,loss 0.000100\n",
      "batch 5192: ,loss 0.109253\n",
      "batch 5193: ,loss 0.000766\n",
      "batch 5194: ,loss 0.000214\n",
      "batch 5195: ,loss 0.001392\n",
      "batch 5196: ,loss 0.001654\n",
      "batch 5197: ,loss 0.000935\n",
      "batch 5198: ,loss 0.002216\n",
      "batch 5199: ,loss 0.000749\n",
      "batch 5200: ,loss 0.061523\n",
      "batch 5201: ,loss 0.000178\n",
      "batch 5202: ,loss 0.002035\n",
      "batch 5203: ,loss 0.004697\n",
      "batch 5204: ,loss 0.000062\n",
      "batch 5205: ,loss 0.207696\n",
      "batch 5206: ,loss 0.000632\n",
      "batch 5207: ,loss 0.000064\n",
      "batch 5208: ,loss 0.000342\n",
      "batch 5209: ,loss 0.004181\n",
      "batch 5210: ,loss 0.000020\n",
      "batch 5211: ,loss 0.014844\n",
      "batch 5212: ,loss 0.004775\n",
      "batch 5213: ,loss 0.005044\n",
      "batch 5214: ,loss 0.009661\n",
      "batch 5215: ,loss 0.001265\n",
      "batch 5216: ,loss 0.039065\n",
      "batch 5217: ,loss 0.003945\n",
      "batch 5218: ,loss 0.015686\n",
      "batch 5219: ,loss 0.017921\n",
      "batch 5220: ,loss 0.001006\n",
      "batch 5221: ,loss 0.049551\n",
      "batch 5222: ,loss 0.000099\n",
      "batch 5223: ,loss 0.074180\n",
      "batch 5224: ,loss 0.070236\n",
      "batch 5225: ,loss 0.001173\n",
      "batch 5226: ,loss 0.014327\n",
      "batch 5227: ,loss 0.000182\n",
      "batch 5228: ,loss 0.007956\n",
      "batch 5229: ,loss 0.001424\n",
      "batch 5230: ,loss 0.000656\n",
      "batch 5231: ,loss 0.002512\n",
      "batch 5232: ,loss 0.040298\n",
      "batch 5233: ,loss 0.001818\n",
      "batch 5234: ,loss 0.042379\n",
      "batch 5235: ,loss 0.004361\n",
      "batch 5236: ,loss 0.003282\n",
      "batch 5237: ,loss 0.002912\n",
      "batch 5238: ,loss 0.000732\n",
      "batch 5239: ,loss 0.003903\n",
      "batch 5240: ,loss 0.010864\n",
      "batch 5241: ,loss 0.005375\n",
      "batch 5242: ,loss 0.107897\n",
      "batch 5243: ,loss 0.002793\n",
      "batch 5244: ,loss 0.005399\n",
      "batch 5245: ,loss 0.000893\n",
      "batch 5246: ,loss 0.091480\n",
      "batch 5247: ,loss 0.002810\n",
      "batch 5248: ,loss 0.001169\n",
      "batch 5249: ,loss 0.110502\n",
      "batch 5250: ,loss 0.000215\n",
      "batch 5251: ,loss 0.014392\n",
      "batch 5252: ,loss 0.011088\n",
      "batch 5253: ,loss 0.003200\n",
      "batch 5254: ,loss 0.001082\n",
      "batch 5255: ,loss 0.000806\n",
      "batch 5256: ,loss 0.012755\n",
      "batch 5257: ,loss 0.001573\n",
      "batch 5258: ,loss 0.002765\n",
      "batch 5259: ,loss 0.007549\n",
      "batch 5260: ,loss 0.002076\n",
      "batch 5261: ,loss 0.013940\n",
      "batch 5262: ,loss 0.000966\n",
      "batch 5263: ,loss 0.000884\n",
      "batch 5264: ,loss 0.000699\n",
      "batch 5265: ,loss 0.000464\n",
      "batch 5266: ,loss 0.000708\n",
      "batch 5267: ,loss 0.000133\n",
      "batch 5268: ,loss 0.000416\n",
      "batch 5269: ,loss 0.012535\n",
      "batch 5270: ,loss 0.004643\n",
      "batch 5271: ,loss 0.000966\n",
      "batch 5272: ,loss 0.002119\n",
      "batch 5273: ,loss 0.001211\n",
      "batch 5274: ,loss 0.045842\n",
      "batch 5275: ,loss 0.003045\n",
      "batch 5276: ,loss 0.000039\n",
      "batch 5277: ,loss 0.000312\n",
      "batch 5278: ,loss 0.001831\n",
      "batch 5279: ,loss 0.001193\n",
      "batch 5280: ,loss 0.000331\n",
      "batch 5281: ,loss 0.002607\n",
      "batch 5282: ,loss 0.007511\n",
      "batch 5283: ,loss 0.000337\n",
      "batch 5284: ,loss 0.000220\n",
      "batch 5285: ,loss 0.001343\n",
      "batch 5286: ,loss 0.003094\n",
      "batch 5287: ,loss 0.000677\n",
      "batch 5288: ,loss 0.013933\n",
      "batch 5289: ,loss 0.010606\n",
      "batch 5290: ,loss 0.016094\n",
      "batch 5291: ,loss 0.017381\n",
      "batch 5292: ,loss 0.001949\n",
      "batch 5293: ,loss 0.000419\n",
      "batch 5294: ,loss 0.000304\n",
      "batch 5295: ,loss 0.077853\n",
      "batch 5296: ,loss 0.081862\n",
      "batch 5297: ,loss 0.008071\n",
      "batch 5298: ,loss 0.004235\n",
      "batch 5299: ,loss 0.002333\n",
      "batch 5300: ,loss 0.004103\n",
      "batch 5301: ,loss 0.007628\n",
      "batch 5302: ,loss 0.053448\n",
      "batch 5303: ,loss 0.009169\n",
      "batch 5304: ,loss 0.000221\n",
      "batch 5305: ,loss 0.060012\n",
      "batch 5306: ,loss 0.278233\n",
      "batch 5307: ,loss 0.000477\n",
      "batch 5308: ,loss 0.004792\n",
      "batch 5309: ,loss 0.001261\n",
      "batch 5310: ,loss 0.027145\n",
      "batch 5311: ,loss 0.077892\n",
      "batch 5312: ,loss 0.027955\n",
      "batch 5313: ,loss 0.008573\n",
      "batch 5314: ,loss 0.010504\n",
      "batch 5315: ,loss 0.061669\n",
      "batch 5316: ,loss 0.001543\n",
      "batch 5317: ,loss 0.000260\n",
      "batch 5318: ,loss 0.000927\n",
      "batch 5319: ,loss 0.000961\n",
      "batch 5320: ,loss 0.000327\n",
      "batch 5321: ,loss 0.011987\n",
      "batch 5322: ,loss 0.002086\n",
      "batch 5323: ,loss 0.075752\n",
      "batch 5324: ,loss 0.078548\n",
      "batch 5325: ,loss 0.033489\n",
      "batch 5326: ,loss 0.004648\n",
      "batch 5327: ,loss 0.000208\n",
      "batch 5328: ,loss 0.023854\n",
      "batch 5329: ,loss 0.000099\n",
      "batch 5330: ,loss 0.001894\n",
      "batch 5331: ,loss 0.024711\n",
      "batch 5332: ,loss 0.009499\n",
      "batch 5333: ,loss 0.012223\n",
      "batch 5334: ,loss 0.001589\n",
      "batch 5335: ,loss 0.002197\n",
      "batch 5336: ,loss 0.009471\n",
      "batch 5337: ,loss 0.039312\n",
      "batch 5338: ,loss 0.050702\n",
      "batch 5339: ,loss 0.002390\n",
      "batch 5340: ,loss 0.018442\n",
      "batch 5341: ,loss 0.107488\n",
      "batch 5342: ,loss 0.054739\n",
      "batch 5343: ,loss 0.000645\n",
      "batch 5344: ,loss 0.000536\n",
      "batch 5345: ,loss 0.001998\n",
      "batch 5346: ,loss 0.001179\n",
      "batch 5347: ,loss 0.000912\n",
      "batch 5348: ,loss 0.003882\n",
      "batch 5349: ,loss 0.013180\n",
      "batch 5350: ,loss 0.004528\n",
      "batch 5351: ,loss 0.006469\n",
      "batch 5352: ,loss 0.013093\n",
      "batch 5353: ,loss 0.001792\n",
      "batch 5354: ,loss 0.001264\n",
      "batch 5355: ,loss 0.001235\n",
      "batch 5356: ,loss 0.003156\n",
      "batch 5357: ,loss 0.007290\n",
      "batch 5358: ,loss 0.000119\n",
      "batch 5359: ,loss 0.001118\n",
      "batch 5360: ,loss 0.012693\n",
      "batch 5361: ,loss 0.002395\n",
      "batch 5362: ,loss 0.002456\n",
      "batch 5363: ,loss 0.002869\n",
      "batch 5364: ,loss 0.000106\n",
      "batch 5365: ,loss 0.000599\n",
      "batch 5366: ,loss 0.004511\n",
      "batch 5367: ,loss 0.001054\n",
      "batch 5368: ,loss 0.004604\n",
      "batch 5369: ,loss 0.000117\n",
      "batch 5370: ,loss 0.001843\n",
      "batch 5371: ,loss 0.000129\n",
      "batch 5372: ,loss 0.035147\n",
      "batch 5373: ,loss 0.005094\n",
      "batch 5374: ,loss 0.000545\n",
      "batch 5375: ,loss 0.000802\n",
      "batch 5376: ,loss 0.000667\n",
      "batch 5377: ,loss 0.001723\n",
      "batch 5378: ,loss 0.000451\n",
      "batch 5379: ,loss 0.000410\n",
      "batch 5380: ,loss 0.000085\n",
      "batch 5381: ,loss 0.000096\n",
      "batch 5382: ,loss 0.000643\n",
      "batch 5383: ,loss 0.004964\n",
      "batch 5384: ,loss 0.025559\n",
      "batch 5385: ,loss 0.001048\n",
      "batch 5386: ,loss 0.005140\n",
      "batch 5387: ,loss 0.000791\n",
      "batch 5388: ,loss 0.001519\n",
      "batch 5389: ,loss 0.004273\n",
      "batch 5390: ,loss 0.101369\n",
      "batch 5391: ,loss 0.028969\n",
      "batch 5392: ,loss 0.000053\n",
      "batch 5393: ,loss 0.000082\n",
      "batch 5394: ,loss 0.091189\n",
      "batch 5395: ,loss 0.001038\n",
      "batch 5396: ,loss 0.003470\n",
      "batch 5397: ,loss 0.000520\n",
      "batch 5398: ,loss 0.006932\n",
      "batch 5399: ,loss 0.000139\n",
      "batch 5400: ,loss 0.000037\n",
      "batch 5401: ,loss 0.000247\n",
      "batch 5402: ,loss 0.000144\n",
      "batch 5403: ,loss 0.001048\n",
      "batch 5404: ,loss 0.002252\n",
      "batch 5405: ,loss 0.002665\n",
      "batch 5406: ,loss 0.003093\n",
      "batch 5407: ,loss 0.000137\n",
      "batch 5408: ,loss 0.012161\n",
      "batch 5409: ,loss 0.001886\n",
      "batch 5410: ,loss 0.000377\n",
      "batch 5411: ,loss 0.002148\n",
      "batch 5412: ,loss 0.019540\n",
      "batch 5413: ,loss 0.000038\n",
      "batch 5414: ,loss 0.000914\n",
      "batch 5415: ,loss 0.000612\n",
      "batch 5416: ,loss 0.005204\n",
      "batch 5417: ,loss 0.021970\n",
      "batch 5418: ,loss 0.000772\n",
      "batch 5419: ,loss 0.010534\n",
      "batch 5420: ,loss 0.001215\n",
      "batch 5421: ,loss 0.000065\n",
      "batch 5422: ,loss 0.000384\n",
      "batch 5423: ,loss 0.000163\n",
      "batch 5424: ,loss 0.000015\n",
      "batch 5425: ,loss 0.000144\n",
      "batch 5426: ,loss 0.000070\n",
      "batch 5427: ,loss 0.000368\n",
      "batch 5428: ,loss 0.221051\n",
      "batch 5429: ,loss 0.000270\n",
      "batch 5430: ,loss 0.001806\n",
      "batch 5431: ,loss 0.000675\n",
      "batch 5432: ,loss 0.000636\n",
      "batch 5433: ,loss 0.000107\n",
      "batch 5434: ,loss 0.001445\n",
      "batch 5435: ,loss 0.000153\n",
      "batch 5436: ,loss 0.047937\n",
      "batch 5437: ,loss 0.000207\n",
      "batch 5438: ,loss 0.009680\n",
      "batch 5439: ,loss 0.069921\n",
      "batch 5440: ,loss 0.000231\n",
      "batch 5441: ,loss 0.003310\n",
      "batch 5442: ,loss 0.029732\n",
      "batch 5443: ,loss 0.007168\n",
      "batch 5444: ,loss 0.004078\n",
      "batch 5445: ,loss 0.000482\n",
      "batch 5446: ,loss 0.000218\n",
      "batch 5447: ,loss 0.000121\n",
      "batch 5448: ,loss 0.000191\n",
      "batch 5449: ,loss 0.009949\n",
      "batch 5450: ,loss 0.002095\n",
      "batch 5451: ,loss 0.034412\n",
      "batch 5452: ,loss 0.000761\n",
      "batch 5453: ,loss 0.000285\n",
      "batch 5454: ,loss 0.001223\n",
      "batch 5455: ,loss 0.001224\n",
      "batch 5456: ,loss 0.026273\n",
      "batch 5457: ,loss 0.000446\n",
      "batch 5458: ,loss 0.005047\n",
      "batch 5459: ,loss 0.007323\n",
      "batch 5460: ,loss 0.000732\n",
      "batch 5461: ,loss 0.006926\n",
      "batch 5462: ,loss 0.046315\n",
      "batch 5463: ,loss 0.002544\n",
      "batch 5464: ,loss 0.000609\n",
      "batch 5465: ,loss 0.001323\n",
      "batch 5466: ,loss 0.002526\n",
      "batch 5467: ,loss 0.006043\n",
      "batch 5468: ,loss 0.012392\n",
      "batch 5469: ,loss 0.000269\n",
      "batch 5470: ,loss 0.029016\n",
      "batch 5471: ,loss 0.001753\n",
      "batch 5472: ,loss 0.000950\n",
      "batch 5473: ,loss 0.000584\n",
      "batch 5474: ,loss 0.004487\n",
      "batch 5475: ,loss 0.034981\n",
      "batch 5476: ,loss 0.001016\n",
      "batch 5477: ,loss 0.049893\n",
      "batch 5478: ,loss 0.000721\n",
      "batch 5479: ,loss 0.000678\n",
      "batch 5480: ,loss 0.000967\n",
      "batch 5481: ,loss 0.000125\n",
      "batch 5482: ,loss 0.035308\n",
      "batch 5483: ,loss 0.044887\n",
      "batch 5484: ,loss 0.001878\n",
      "batch 5485: ,loss 0.030488\n",
      "batch 5486: ,loss 0.068926\n",
      "batch 5487: ,loss 0.003437\n",
      "batch 5488: ,loss 0.004293\n",
      "batch 5489: ,loss 0.029837\n",
      "batch 5490: ,loss 0.000242\n",
      "batch 5491: ,loss 0.000063\n",
      "batch 5492: ,loss 0.000520\n",
      "batch 5493: ,loss 0.000228\n",
      "batch 5494: ,loss 0.001117\n",
      "batch 5495: ,loss 0.001151\n",
      "batch 5496: ,loss 0.000287\n",
      "batch 5497: ,loss 0.017701\n",
      "batch 5498: ,loss 0.007336\n",
      "batch 5499: ,loss 0.000133\n",
      "batch 5500: ,loss 0.000126\n",
      "batch 5501: ,loss 0.000084\n",
      "batch 5502: ,loss 0.003047\n",
      "batch 5503: ,loss 0.012623\n",
      "batch 5504: ,loss 0.008848\n",
      "batch 5505: ,loss 0.005079\n",
      "batch 5506: ,loss 0.003921\n",
      "batch 5507: ,loss 0.000132\n",
      "batch 5508: ,loss 0.007725\n",
      "batch 5509: ,loss 0.015884\n",
      "batch 5510: ,loss 0.005791\n",
      "batch 5511: ,loss 0.003327\n",
      "batch 5512: ,loss 0.007472\n",
      "batch 5513: ,loss 0.000377\n",
      "batch 5514: ,loss 0.003499\n",
      "batch 5515: ,loss 0.000702\n",
      "batch 5516: ,loss 0.012542\n",
      "batch 5517: ,loss 0.001453\n",
      "batch 5518: ,loss 0.010051\n",
      "batch 5519: ,loss 0.013963\n",
      "batch 5520: ,loss 0.001143\n",
      "batch 5521: ,loss 0.006309\n",
      "batch 5522: ,loss 0.005292\n",
      "batch 5523: ,loss 0.000102\n",
      "batch 5524: ,loss 0.028345\n",
      "batch 5525: ,loss 0.000514\n",
      "batch 5526: ,loss 0.000031\n",
      "batch 5527: ,loss 0.000192\n",
      "batch 5528: ,loss 0.000983\n",
      "batch 5529: ,loss 0.000021\n",
      "batch 5530: ,loss 0.000330\n",
      "batch 5531: ,loss 0.000279\n",
      "batch 5532: ,loss 0.005151\n",
      "batch 5533: ,loss 0.001478\n",
      "batch 5534: ,loss 0.018681\n",
      "batch 5535: ,loss 0.000633\n",
      "batch 5536: ,loss 0.035731\n",
      "batch 5537: ,loss 0.000118\n",
      "batch 5538: ,loss 0.000066\n",
      "batch 5539: ,loss 0.000197\n",
      "batch 5540: ,loss 0.008662\n",
      "batch 5541: ,loss 0.188403\n",
      "batch 5542: ,loss 0.004072\n",
      "batch 5543: ,loss 0.000019\n",
      "batch 5544: ,loss 0.000116\n",
      "batch 5545: ,loss 0.026363\n",
      "batch 5546: ,loss 0.000306\n",
      "batch 5547: ,loss 0.004243\n",
      "batch 5548: ,loss 0.006687\n",
      "batch 5549: ,loss 0.000844\n",
      "batch 5550: ,loss 0.000056\n",
      "batch 5551: ,loss 0.015310\n",
      "batch 5552: ,loss 0.000129\n",
      "batch 5553: ,loss 0.001293\n",
      "batch 5554: ,loss 0.000060\n",
      "batch 5555: ,loss 0.006897\n",
      "batch 5556: ,loss 0.000635\n",
      "batch 5557: ,loss 0.012943\n",
      "batch 5558: ,loss 0.003077\n",
      "batch 5559: ,loss 0.069541\n",
      "batch 5560: ,loss 0.000111\n",
      "batch 5561: ,loss 0.000975\n",
      "batch 5562: ,loss 0.001029\n",
      "batch 5563: ,loss 0.000132\n",
      "batch 5564: ,loss 0.002947\n",
      "batch 5565: ,loss 0.000188\n",
      "batch 5566: ,loss 0.000079\n",
      "batch 5567: ,loss 0.016007\n",
      "batch 5568: ,loss 0.000067\n",
      "batch 5569: ,loss 0.000114\n",
      "batch 5570: ,loss 0.000700\n",
      "batch 5571: ,loss 0.060548\n",
      "batch 5572: ,loss 0.134648\n",
      "batch 5573: ,loss 0.000463\n",
      "batch 5574: ,loss 0.003097\n",
      "batch 5575: ,loss 0.001018\n",
      "batch 5576: ,loss 0.012990\n",
      "batch 5577: ,loss 0.037357\n",
      "batch 5578: ,loss 0.000555\n",
      "batch 5579: ,loss 0.007389\n",
      "batch 5580: ,loss 0.003048\n",
      "batch 5581: ,loss 0.007997\n",
      "batch 5582: ,loss 0.000467\n",
      "batch 5583: ,loss 0.014807\n",
      "batch 5584: ,loss 0.042728\n",
      "batch 5585: ,loss 0.004051\n",
      "batch 5586: ,loss 0.005353\n",
      "batch 5587: ,loss 0.002462\n",
      "batch 5588: ,loss 0.000633\n",
      "batch 5589: ,loss 0.006933\n",
      "batch 5590: ,loss 0.004964\n",
      "batch 5591: ,loss 0.000531\n",
      "batch 5592: ,loss 0.001005\n",
      "batch 5593: ,loss 0.000166\n",
      "batch 5594: ,loss 0.006285\n",
      "batch 5595: ,loss 0.000093\n",
      "batch 5596: ,loss 0.000478\n",
      "batch 5597: ,loss 0.025027\n",
      "batch 5598: ,loss 0.000319\n",
      "batch 5599: ,loss 0.082914\n",
      "batch 5600: ,loss 0.000900\n",
      "batch 5601: ,loss 0.001025\n",
      "batch 5602: ,loss 0.004209\n",
      "batch 5603: ,loss 0.000089\n",
      "batch 5604: ,loss 0.028661\n",
      "batch 5605: ,loss 0.000698\n",
      "batch 5606: ,loss 0.000014\n",
      "batch 5607: ,loss 0.000080\n",
      "batch 5608: ,loss 0.007749\n",
      "batch 5609: ,loss 0.004560\n",
      "batch 5610: ,loss 0.036854\n",
      "batch 5611: ,loss 0.000573\n",
      "batch 5612: ,loss 0.000012\n",
      "batch 5613: ,loss 0.002045\n",
      "batch 5614: ,loss 0.000502\n",
      "batch 5615: ,loss 0.000109\n",
      "batch 5616: ,loss 0.000023\n",
      "batch 5617: ,loss 0.000068\n",
      "batch 5618: ,loss 0.000121\n",
      "batch 5619: ,loss 0.003516\n",
      "batch 5620: ,loss 0.012092\n",
      "batch 5621: ,loss 0.000337\n",
      "batch 5622: ,loss 0.002666\n",
      "batch 5623: ,loss 0.000054\n",
      "batch 5624: ,loss 0.000414\n",
      "batch 5625: ,loss 0.000195\n",
      "batch 5626: ,loss 0.044024\n",
      "batch 5627: ,loss 0.028449\n",
      "batch 5628: ,loss 0.002821\n",
      "batch 5629: ,loss 0.000228\n",
      "batch 5630: ,loss 0.000659\n",
      "batch 5631: ,loss 0.003881\n",
      "batch 5632: ,loss 0.000274\n",
      "batch 5633: ,loss 0.000034\n",
      "batch 5634: ,loss 0.010751\n",
      "batch 5635: ,loss 0.000784\n",
      "batch 5636: ,loss 0.000057\n",
      "batch 5637: ,loss 0.000088\n",
      "batch 5638: ,loss 0.014997\n",
      "batch 5639: ,loss 0.005446\n",
      "batch 5640: ,loss 0.002126\n",
      "batch 5641: ,loss 0.000137\n",
      "batch 5642: ,loss 0.013850\n",
      "batch 5643: ,loss 0.002558\n",
      "batch 5644: ,loss 0.002233\n",
      "batch 5645: ,loss 0.064797\n",
      "batch 5646: ,loss 0.004187\n",
      "batch 5647: ,loss 0.003491\n",
      "batch 5648: ,loss 0.001031\n",
      "batch 5649: ,loss 0.000568\n",
      "batch 5650: ,loss 0.000215\n",
      "batch 5651: ,loss 0.000604\n",
      "batch 5652: ,loss 0.000066\n",
      "batch 5653: ,loss 0.018324\n",
      "batch 5654: ,loss 0.004372\n",
      "batch 5655: ,loss 0.000016\n",
      "batch 5656: ,loss 0.012746\n",
      "batch 5657: ,loss 0.001054\n",
      "batch 5658: ,loss 0.000394\n",
      "batch 5659: ,loss 0.000069\n",
      "batch 5660: ,loss 0.004562\n",
      "batch 5661: ,loss 0.002335\n",
      "batch 5662: ,loss 0.009693\n",
      "batch 5663: ,loss 0.000277\n",
      "batch 5664: ,loss 0.002798\n",
      "batch 5665: ,loss 0.070040\n",
      "batch 5666: ,loss 0.000052\n",
      "batch 5667: ,loss 0.000311\n",
      "batch 5668: ,loss 0.000819\n",
      "batch 5669: ,loss 0.006702\n",
      "batch 5670: ,loss 0.036508\n",
      "batch 5671: ,loss 0.000595\n",
      "batch 5672: ,loss 0.005063\n",
      "batch 5673: ,loss 0.034419\n",
      "batch 5674: ,loss 0.000513\n",
      "batch 5675: ,loss 0.000078\n",
      "batch 5676: ,loss 0.000595\n",
      "batch 5677: ,loss 0.000198\n",
      "batch 5678: ,loss 0.000236\n",
      "batch 5679: ,loss 0.000744\n",
      "batch 5680: ,loss 0.063761\n",
      "batch 5681: ,loss 0.000647\n",
      "batch 5682: ,loss 0.002656\n",
      "batch 5683: ,loss 0.001002\n",
      "batch 5684: ,loss 0.001048\n",
      "batch 5685: ,loss 0.007395\n",
      "batch 5686: ,loss 0.000657\n",
      "batch 5687: ,loss 0.000073\n",
      "batch 5688: ,loss 0.000036\n",
      "batch 5689: ,loss 0.057062\n",
      "batch 5690: ,loss 0.000057\n",
      "batch 5691: ,loss 0.000036\n",
      "batch 5692: ,loss 0.033286\n",
      "batch 5693: ,loss 0.000383\n",
      "batch 5694: ,loss 0.176564\n",
      "batch 5695: ,loss 0.000059\n",
      "batch 5696: ,loss 0.000315\n",
      "batch 5697: ,loss 0.003070\n",
      "batch 5698: ,loss 0.000133\n",
      "batch 5699: ,loss 0.006813\n",
      "batch 5700: ,loss 0.021651\n",
      "batch 5701: ,loss 0.000126\n",
      "batch 5702: ,loss 0.012444\n",
      "batch 5703: ,loss 0.000316\n",
      "batch 5704: ,loss 0.000500\n",
      "batch 5705: ,loss 0.001772\n",
      "batch 5706: ,loss 0.009958\n",
      "batch 5707: ,loss 0.072782\n",
      "batch 5708: ,loss 0.000335\n",
      "batch 5709: ,loss 0.008229\n",
      "batch 5710: ,loss 0.000241\n",
      "batch 5711: ,loss 0.005497\n",
      "batch 5712: ,loss 0.003536\n",
      "batch 5713: ,loss 0.002449\n",
      "batch 5714: ,loss 0.000372\n",
      "batch 5715: ,loss 0.003105\n",
      "batch 5716: ,loss 0.000644\n",
      "batch 5717: ,loss 0.001330\n",
      "batch 5718: ,loss 0.008301\n",
      "batch 5719: ,loss 0.000222\n",
      "batch 5720: ,loss 0.000981\n",
      "batch 5721: ,loss 0.000728\n",
      "batch 5722: ,loss 0.004079\n",
      "batch 5723: ,loss 0.016411\n",
      "batch 5724: ,loss 0.000554\n",
      "batch 5725: ,loss 0.004606\n",
      "batch 5726: ,loss 0.001260\n",
      "batch 5727: ,loss 0.000381\n",
      "batch 5728: ,loss 0.004580\n",
      "batch 5729: ,loss 0.001174\n",
      "batch 5730: ,loss 0.010042\n",
      "batch 5731: ,loss 0.005082\n",
      "batch 5732: ,loss 0.000652\n",
      "batch 5733: ,loss 0.000287\n",
      "batch 5734: ,loss 0.001299\n",
      "batch 5735: ,loss 0.000177\n",
      "batch 5736: ,loss 0.002893\n",
      "batch 5737: ,loss 0.002901\n",
      "batch 5738: ,loss 0.000245\n",
      "batch 5739: ,loss 0.000070\n",
      "batch 5740: ,loss 0.000242\n",
      "batch 5741: ,loss 0.000135\n",
      "batch 5742: ,loss 0.000120\n",
      "batch 5743: ,loss 0.001519\n",
      "batch 5744: ,loss 0.001688\n",
      "batch 5745: ,loss 0.007996\n",
      "batch 5746: ,loss 0.000500\n",
      "batch 5747: ,loss 0.000880\n",
      "batch 5748: ,loss 0.000076\n",
      "batch 5749: ,loss 0.001188\n",
      "batch 5750: ,loss 0.008570\n",
      "batch 5751: ,loss 0.000076\n",
      "batch 5752: ,loss 0.001687\n",
      "batch 5753: ,loss 0.000170\n",
      "batch 5754: ,loss 0.001204\n",
      "batch 5755: ,loss 0.000261\n",
      "batch 5756: ,loss 0.102027\n",
      "batch 5757: ,loss 0.001892\n",
      "batch 5758: ,loss 0.070921\n",
      "batch 5759: ,loss 0.000197\n",
      "batch 5760: ,loss 0.000329\n",
      "batch 5761: ,loss 0.000668\n",
      "batch 5762: ,loss 0.006906\n",
      "batch 5763: ,loss 0.049031\n",
      "batch 5764: ,loss 0.003365\n",
      "batch 5765: ,loss 0.018266\n",
      "batch 5766: ,loss 0.000337\n",
      "batch 5767: ,loss 0.004642\n",
      "batch 5768: ,loss 0.112690\n",
      "batch 5769: ,loss 0.000024\n",
      "batch 5770: ,loss 0.000445\n",
      "batch 5771: ,loss 0.000228\n",
      "batch 5772: ,loss 0.000121\n",
      "batch 5773: ,loss 0.000128\n",
      "batch 5774: ,loss 0.000820\n",
      "batch 5775: ,loss 0.012705\n",
      "batch 5776: ,loss 0.000018\n",
      "batch 5777: ,loss 0.179810\n",
      "batch 5778: ,loss 0.000920\n",
      "batch 5779: ,loss 0.011393\n",
      "batch 5780: ,loss 0.001242\n",
      "batch 5781: ,loss 0.012536\n",
      "batch 5782: ,loss 0.000773\n",
      "batch 5783: ,loss 0.000225\n",
      "batch 5784: ,loss 0.000122\n",
      "batch 5785: ,loss 0.000537\n",
      "batch 5786: ,loss 0.010823\n",
      "batch 5787: ,loss 0.004015\n",
      "batch 5788: ,loss 0.000138\n",
      "batch 5789: ,loss 0.000147\n",
      "batch 5790: ,loss 0.035739\n",
      "batch 5791: ,loss 0.018509\n",
      "batch 5792: ,loss 0.000575\n",
      "batch 5793: ,loss 0.000308\n",
      "batch 5794: ,loss 0.007553\n",
      "batch 5795: ,loss 0.007050\n",
      "batch 5796: ,loss 0.013938\n",
      "batch 5797: ,loss 0.003541\n",
      "batch 5798: ,loss 0.180375\n",
      "batch 5799: ,loss 0.047971\n",
      "batch 5800: ,loss 0.000423\n",
      "batch 5801: ,loss 0.000035\n",
      "batch 5802: ,loss 0.123634\n",
      "batch 5803: ,loss 0.002087\n",
      "batch 5804: ,loss 0.000744\n",
      "batch 5805: ,loss 0.001031\n",
      "batch 5806: ,loss 0.003643\n",
      "batch 5807: ,loss 0.007412\n",
      "batch 5808: ,loss 0.001977\n",
      "batch 5809: ,loss 0.000723\n",
      "batch 5810: ,loss 0.003351\n",
      "batch 5811: ,loss 0.000043\n",
      "batch 5812: ,loss 0.027580\n",
      "batch 5813: ,loss 0.000125\n",
      "batch 5814: ,loss 0.033706\n",
      "batch 5815: ,loss 0.004138\n",
      "batch 5816: ,loss 0.020244\n",
      "batch 5817: ,loss 0.000129\n",
      "batch 5818: ,loss 0.000148\n",
      "batch 5819: ,loss 0.001656\n",
      "batch 5820: ,loss 0.004682\n",
      "batch 5821: ,loss 0.005755\n",
      "batch 5822: ,loss 0.006106\n",
      "batch 5823: ,loss 0.001954\n",
      "batch 5824: ,loss 0.019188\n",
      "batch 5825: ,loss 0.019456\n",
      "batch 5826: ,loss 0.086878\n",
      "batch 5827: ,loss 0.004305\n",
      "batch 5828: ,loss 0.001054\n",
      "batch 5829: ,loss 0.000116\n",
      "batch 5830: ,loss 0.002684\n",
      "batch 5831: ,loss 0.001910\n",
      "batch 5832: ,loss 0.000712\n",
      "batch 5833: ,loss 0.015920\n",
      "batch 5834: ,loss 0.000321\n",
      "batch 5835: ,loss 0.000420\n",
      "batch 5836: ,loss 0.010358\n",
      "batch 5837: ,loss 0.011457\n",
      "batch 5838: ,loss 0.000017\n",
      "batch 5839: ,loss 0.000506\n",
      "batch 5840: ,loss 0.029819\n",
      "batch 5841: ,loss 0.004383\n",
      "batch 5842: ,loss 0.000427\n",
      "batch 5843: ,loss 0.003260\n",
      "batch 5844: ,loss 0.058096\n",
      "batch 5845: ,loss 0.001695\n",
      "batch 5846: ,loss 0.001312\n",
      "batch 5847: ,loss 0.002143\n",
      "batch 5848: ,loss 0.000316\n",
      "batch 5849: ,loss 0.000140\n",
      "batch 5850: ,loss 0.000068\n",
      "batch 5851: ,loss 0.000376\n",
      "batch 5852: ,loss 0.000037\n",
      "batch 5853: ,loss 0.001841\n",
      "batch 5854: ,loss 0.000429\n",
      "batch 5855: ,loss 0.000515\n",
      "batch 5856: ,loss 0.000114\n",
      "batch 5857: ,loss 0.000626\n",
      "batch 5858: ,loss 0.002634\n",
      "batch 5859: ,loss 0.003112\n",
      "batch 5860: ,loss 0.006073\n",
      "batch 5861: ,loss 0.001414\n",
      "batch 5862: ,loss 0.098803\n",
      "batch 5863: ,loss 0.001471\n",
      "batch 5864: ,loss 0.001642\n",
      "batch 5865: ,loss 0.005047\n",
      "batch 5866: ,loss 0.000087\n",
      "batch 5867: ,loss 0.004832\n",
      "batch 5868: ,loss 0.019689\n",
      "batch 5869: ,loss 0.000080\n",
      "batch 5870: ,loss 0.000115\n",
      "batch 5871: ,loss 0.000440\n",
      "batch 5872: ,loss 0.000451\n",
      "batch 5873: ,loss 0.006820\n",
      "batch 5874: ,loss 0.122116\n",
      "batch 5875: ,loss 0.001526\n",
      "batch 5876: ,loss 0.036476\n",
      "batch 5877: ,loss 0.000119\n",
      "batch 5878: ,loss 0.005467\n",
      "batch 5879: ,loss 0.000514\n",
      "batch 5880: ,loss 0.007687\n",
      "batch 5881: ,loss 0.000133\n",
      "batch 5882: ,loss 0.001415\n",
      "batch 5883: ,loss 0.004142\n",
      "batch 5884: ,loss 0.000171\n",
      "batch 5885: ,loss 0.000029\n",
      "batch 5886: ,loss 0.000022\n",
      "batch 5887: ,loss 0.000649\n",
      "batch 5888: ,loss 0.066248\n",
      "batch 5889: ,loss 0.000071\n",
      "batch 5890: ,loss 0.026036\n",
      "batch 5891: ,loss 0.000357\n",
      "batch 5892: ,loss 0.001825\n",
      "batch 5893: ,loss 0.000169\n",
      "batch 5894: ,loss 0.004822\n",
      "batch 5895: ,loss 0.002504\n",
      "batch 5896: ,loss 0.053143\n",
      "batch 5897: ,loss 0.000111\n",
      "batch 5898: ,loss 0.007119\n",
      "batch 5899: ,loss 0.002276\n",
      "batch 5900: ,loss 0.006307\n",
      "batch 5901: ,loss 0.001390\n",
      "batch 5902: ,loss 0.014983\n",
      "batch 5903: ,loss 0.030104\n",
      "batch 5904: ,loss 0.017957\n",
      "batch 5905: ,loss 0.000379\n",
      "batch 5906: ,loss 0.033074\n",
      "batch 5907: ,loss 0.002468\n",
      "batch 5908: ,loss 0.001032\n",
      "batch 5909: ,loss 0.002028\n",
      "batch 5910: ,loss 0.007535\n",
      "batch 5911: ,loss 0.015325\n",
      "batch 5912: ,loss 0.006020\n",
      "batch 5913: ,loss 0.000154\n",
      "batch 5914: ,loss 0.003921\n",
      "batch 5915: ,loss 0.004376\n",
      "batch 5916: ,loss 0.000561\n",
      "batch 5917: ,loss 0.000171\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-764174b97bee>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"batch %d: ,loss %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m     \u001b[0mgrads\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\_binghan_github\\ML_Notes\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[1;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[0;32m   1071\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1073\u001b[1;33m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\_binghan_github\\ML_Notes\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[1;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[0;32m     75\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     76\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 77\u001b[1;33m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
      "\u001b[1;32md:\\_binghan_github\\ML_Notes\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\eager\\backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[1;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[0;32m    160\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m\"/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 162\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    163\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    164\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\_binghan_github\\ML_Notes\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\nn_grad.py\u001b[0m in \u001b[0;36m_Conv2DGrad\u001b[1;34m(op, grad)\u001b[0m\n\u001b[0;32m    604\u001b[0m           \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 606\u001b[1;33m           data_format=data_format)\n\u001b[0m\u001b[0;32m    607\u001b[0m   ]\n\u001b[0;32m    608\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\_binghan_github\\ML_Notes\\tensorflow2\\lib\\site-packages\\tensorflow\\python\\ops\\gen_nn_ops.py\u001b[0m in \u001b[0;36mconv2d_backprop_filter\u001b[1;34m(input, filter_sizes, out_backprop, strides, padding, use_cudnn_on_gpu, explicit_paddings, data_format, dilations, name)\u001b[0m\n\u001b[0;32m   1089\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"use_cudnn_on_gpu\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_cudnn_on_gpu\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"padding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1090\u001b[0m         \u001b[1;34m\"explicit_paddings\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexplicit_paddings\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"data_format\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1091\u001b[1;33m         \"dilations\", dilations)\n\u001b[0m\u001b[0;32m   1092\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1093\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_batches = int(data_loader.num_train_data // batch_size * num_epochs)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    X, y = data_loader.get_batch(batch_size)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        y_pred = model(X)\n",
    "        loss = tf.keras.losses.sparse_categorical_crossentropy(y_true=y, y_pred=y_pred)\n",
    "        loss = tf.reduce_mean(loss)\n",
    "        print(\"batch %d: ,loss %f\" % (batch_index, loss.numpy()))\n",
    "\n",
    "    grads = tape.gradient(loss, model.variables)\n",
    "    optimizer.apply_gradients(grads_and_vars=zip(grads, model.variables))"
   ]
  },
  {
   "source": [
    "## 模型的評估"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "test accuracy: 0.992100\n"
     ]
    }
   ],
   "source": [
    "sparse_categorical_accuracy = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "num_batches = int(data_loader.num_test_data // batch_size)\n",
    "\n",
    "for batch_index in range(num_batches):\n",
    "    start_index, end_index = batch_index * batch_size, (batch_index + 1) * batch_size\n",
    "    y_pred = model.predict(data_loader.test_data[start_index: end_index])\n",
    "    sparse_categorical_accuracy.update_state(y_true=data_loader.test_label[start_index: end_index], y_pred=y_pred)\n",
    "    \n",
    "print(\"test accuracy: %f\" % sparse_categorical_accuracy.result())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pythonjvsc74a57bd001ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f",
   "display_name": "Python 3.7.5  ('tensorflow2': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "01ab250d51d476354f881459c2d06896efccfd5024e6637054e3702514e5975f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}